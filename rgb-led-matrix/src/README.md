Note to self: There currently is one thread for the TCP socket, one listener thread per data node, two compute threads and one IO thread. 

It would be nice to get this down to two threads globally (one for TCP socket and one listener for all data nodes in the domain), two compute threads and one IO thread.

My concern is we may end up racing to prevent scaling issues. The listener threads effectively go single threaded. Meaning latency with size. I would need to nest to create larger panels. In theory I support this. The issue is we risk sync lock up with deep hierarchy. My personal project will likely never care.

## Protocol Design
Note you can remove the sync control protocol with a custom protocol which bypasses this feature intentionally. However this is only recommended on async networks which for LED displays is not recommended. These networks should be high bandwidth. Nesting is used for managing bandwidth. 

Reminder: Sync is what you use on computation, displays, audio, power supplies, communications, etc. Async is a substep of larger SISD synchronous system. Async may be MISD, MIMD or SIMD. Multiple ways to implement this: Scalar, Matrix, CMT, pipeling, etc. Most async is divided into two groups: polling and event driven. However most are polling and usually without priority. Error detection and recovery are hard without a synchronous network. Core network failure is problematic but can be handled by delegation or deferment. Async operations are in order of SISD dispatch. Preemption is a higher level notion which is handled by OS using multi-processing with priority/CoS/QoS. (We do not have an efficient notion of this currently! Someday we may get a proper notion of threads.) Currently we have FPGA processing which uses linear resource expansion to manage things. Therefore our ability to manage priority and errors are constrained by design. This limits our performance and complexity for the time being. The main force driving this conflict is networking performance against cyber-security, which will require a new convention for managing risk/trust.

### RP2040 UART Protocol Design
RP2040 is bandwidth constrained so we use a control network which has high bandwidth with a smaller token. These packets are submitted in parallel at a low level of effort. Host should be capable of implmenting an error detection protocol. (In the future a time out to blank feature can be implemented in the RP2040.) The intention of this control network is to allow synchronization at a high refresh rate.

### RPi High Level Protocol Design
RPi is less bandwidth constrained so we can bypass the control network. This creates a race condition and we just brute force the refresh rate. We must support a frame rate at or greater than refresh rate on the network. (In the future a time out to blank feature can be implemented in the RPi.) Note the RPi may be a unification of multiple RP2040 into a single display. There is a limit to how many RP2040 per RPi can exist. This limit is believed to be the single threaded performance bottleneck. (I am not confident in the OS managing this single threaded performance using mutliple threads.)